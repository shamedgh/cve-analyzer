import os, sys, subprocess, signal
import logging
import optparse
import cveweb
import nvdweb 
import json
from decimal import Decimal
from base64 import b64encode, b64decode
from json import dumps, loads, JSONEncoder
import pickle

sys.path.insert(0, './python-utils')

import util
import scraper

class PythonObjectEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (list, dict, str, int, float, bool, type(None))):
            return super().default(obj)
        return {'_python_object': b64encode(pickle.dumps(obj)).decode('utf-8')}

#def as_python_object(dct):
#    if '_python_object' in dct:
#        return pickle.loads(b64decode(dct['_python_object'].encode('utf-8')))
#    return dct

#class SetEncoder(json.JSONEncoder):
#    def default(self, obj):
#        if isinstance(obj, set):
#            return list(obj)
#        return json.JSONEncoder.default(self, obj)

def isValidOpts(opts):
    """
    Check if the required options are sane to be accepted
        - Check if the provided files exist
        - Check if two sections (additional data) exist
        - Read all target libraries to be debloated from the provided list
    :param opts:
    :return:
    """
    if not options.url:
        parser.error("Option -u should be provided.")
        return False
    return True

def getCveFromUrl(url):
    if ( "cvedetails.com" in url ):
        splittedUrl = url.split("/")
        if ( len(splittedUrl) >= 5 ):
            return splittedUrl[4]
    elif ( "nvd.nist.gov" in url ):
        splittedUrl = url.split("/")
        if ( len(splittedUrl) >= 6 ):
            return splittedUrl[5]

def setLogPath(logPath):
    """
    Set the property of the logger: path, config, and format
    :param logPath:
    :return:
    """
    if os.path.exists(logPath):
        os.remove(logPath)

    rootLogger = logging.getLogger("coverage")
    if options.debug:
        logging.basicConfig(filename=logPath, level=logging.DEBUG)
        rootLogger.setLevel(logging.DEBUG)
    else:
        logging.basicConfig(filename=logPath, level=logging.INFO)
        rootLogger.setLevel(logging.INFO)

#    ch = logging.StreamHandler(sys.stdout)
    consoleHandler = logging.StreamHandler()
    rootLogger.addHandler(consoleHandler)
    return rootLogger
#    rootLogger.addHandler(ch)

if __name__ == '__main__':
    """
    Main function for finding physical memory usage of process
    """
    #data = [1,2,3, set(['knights', 'who', 'say', 'ni'])]
    #data = {"cve":{"commitId":{"fileId":[1000,2000,3000]}}}
    #j = dumps(data, cls=PythonObjectEncoder)
    #print(loads(j, object_hook=as_python_object))

    usage = "Usage: %prog -e <Target executable path> -p <PID of process to retrieve information about>"

    parser = optparse.OptionParser(usage=usage, version="1")

    parser.add_option("-u", "--url", dest="url", default=None, nargs=1,
                      help="Url to run scraper on")

    parser.add_option("-n", "--totalpages", dest="totalpages", default=1, nargs=1,
                      help="Total page numbers for search")

    parser.add_option("-o", "--output", dest="output", default=1, nargs=1,
                      help="CVE dictionary output file path")

    parser.add_option("-d", "--debug", dest="debug", action="store_true", default=False,
                      help="Debug enabled/disabled")

    (options, args) = parser.parse_args()
    if isValidOpts(options):
        rootLogger = setLogPath("cvetofilemapper.log")
        urlList = []
        i = 1
        while ( i < int(options.totalpages) ):
        #for i in range(1, int(options.totalpages)):
            url = options.url.format(i)
            scraperObj = scraper.Scraper(url, rootLogger)
            rootLogger.info("url: %s website url: %s", url, scraperObj.getWebsiteFromUrl())
            filterList = ["cve/", "detail/CVE"]
            soupObj = scraperObj.loadWebsite()
            if ( soupObj ):
                websiteUrl = scraperObj.getWebsiteFromUrl()
                urlList.extend(util.htmlParseExtractLinks(soupObj, websiteUrl, filterList))
                rootLogger.info("length of urlList: %d", len(urlList))
            rootLogger.info("i in for loop: %d", i)
            if ( "nvd.nist.gov" in url ):
                i += 20
            else:
                i += 1

        rootLogger.info("Finished extracting the CVE details page for each CVE")
        rootLogger.debug("urlList: %s", str(urlList))

        outputFile = open(options.output, 'w')

        filterList = ["git.kernel.org"]
        cveIds = set()
        cveIdsWoCommits = set()

        mainDict = dict()
        cveToInfoDict = dict()

        for url in urlList:
            scraperObj = scraper.Scraper(url, rootLogger)
            cveId = getCveFromUrl(url)
            cveIds.add(cveId)
            soupObj = scraperObj.loadWebsite()
            if ( soupObj ):
                websiteUrl = scraperObj.getWebsiteFromUrl()

                #Extract extra CVE properties, score, type
                if ( "cvedetails" in url ): # If extracting CVE info from cvedetails.com
                    soupObj = soupObj.find("table", {"class": "details", "id": "cvssscorestable"})
                    if ( soupObj ):
                        cveScore = cveweb.extractCveScore(soupObj, rootLogger)
                        cveVulnType = cveweb.extractCveVulnType(soupObj, rootLogger)
                        cveInfoDict = cveToInfoDict.get(cveId, dict())
                        cveInfoDict["score"] = cveScore
                        cveInfoDict["vulntype"] = str(cveVulnType)
                        cveToInfoDict[cveId] = cveInfoDict

                    soupObj = soupObj.find("table", {"class": "listtable", "id": "vulnrefstable"})
                    if ( soupObj ):
                        gitCommitUrls = util.htmlParseExtractLinks(soupObj, websiteUrl, filterList)
                        rootLogger.debug("Finished extracting gitCommitUrls for CVE page: %s", str(gitCommitUrls))
                        if ( len(gitCommitUrls) == 0 ):
                            cveIdsWoCommits.add(cveId)
                        for gitCommitUrl in gitCommitUrls:
                            rootLogger.debug("Extracting CVE details from: %s", gitCommitUrl)
                            cvewebObj = cveweb.CVEWeb(gitCommitUrl, rootLogger)
                            tempDict = mainDict.get(cveId, dict())
                            currentDict = cvewebObj.getKernelGitCommits()
                            for currCommitId, currFileLineDict in currentDict.items():
                                if ( currCommitId in tempDict.keys() ):
                                    for currFile, currLineSet in currFileLineDict.items():
                                        if ( currFile in tempDict[currCommitId].keys() ):
                                            tempDict[currCommitId][currFile].update(currLineSet)
                                        else:
                                            tempDict[currCommitId][currFile] = currLineSet
                                else:
                                    tempDict[currCommitId] = currFileLineDict
                            mainDict[cveId] = tempDict
                elif ( "nvd.nist.gov" in url ): # If extracting CVE info from nvd.nist.gov
                    severitySoupObj = soupObj.find("div", {"class": "container-fluid", "id": "Vuln3CvssPanel"})
                    if ( severitySoupObj ):
                        cveScore = nvdweb.extractCveScore(severitySoupObj, rootLogger)
                        cveVulnType = nvdweb.extractCveVulnType(severitySoupObj, rootLogger)
                        cveInfoDict = cveToInfoDict.get(cveId, dict())
                        cveInfoDict["score"] = cveScore
                        cveInfoDict["vulntype"] = str(cveVulnType)
                        cveToInfoDict[cveId] = cveInfoDict

                    linkSoupObj = soupObj.find("table", {"data-testid": "vuln-hyperlinks-table"})
                    if ( linkSoupObj ):
                        gitCommitUrls = util.htmlParseExtractLinks(linkSoupObj, websiteUrl, filterList)
                        rootLogger.debug("Finished extracting gitCommitUrls for CVE page: %s", str(gitCommitUrls))
                        if ( len(gitCommitUrls) == 0 ):
                            cveIdsWoCommits.add(cveId)
                        for gitCommitUrl in gitCommitUrls:
                            rootLogger.debug("Extracting CVE details from: %s", gitCommitUrl)
                            cvewebObj = cveweb.CVEWeb(gitCommitUrl, rootLogger)
                            tempDict = mainDict.get(cveId, dict())
                            currentDict = cvewebObj.getKernelGitCommits()
                            for currCommitId, currFileLineDict in currentDict.items():
                                if ( currCommitId in tempDict.keys() ):
                                    for currFile, currLineSet in currFileLineDict.items():
                                        if ( currFile in tempDict[currCommitId].keys() ):
                                            tempDict[currCommitId][currFile].update(currLineSet)
                                        else:
                                            tempDict[currCommitId][currFile] = currLineSet
                                else:
                                    tempDict[currCommitId] = currFileLineDict
                            mainDict[cveId] = tempDict
        #json = json.dumps(mainDict)
        json = dumps(mainDict, cls=PythonObjectEncoder)
        #json = dumps(mainDict, cls=SetEncoder)
        outputFile.write(json)
        outputFile.close()

        cveScoreOutputFile = open(options.output + ".scores.csv", 'w')
        cveTypeOutputFile = open(options.output + ".type.csv", 'w')

        for cveId, cveInfoDict in cveToInfoDict.items():
            score = cveInfoDict["score"]
            vulnType = cveInfoDict["vulntype"]
            cveScoreOutputFile.write(cveId + ";" + score + "\n")
            cveScoreOutputFile.flush()
            cveTypeOutputFile.write(cveId + ";" + vulnType + "\n")
            cveTypeOutputFile.flush()

        cveScoreOutputFile.close()
        cveTypeOutputFile.close()

        rootLogger.info("Finished extracting CVE details for %d cves, %d cves didn't have any commits", len(cveIds), len(cveIdsWoCommits))
        rootLogger.info("CVEs without commits: %s", str(cveIdsWoCommits))
#                    rootLogger.info("cveFileLineDict: %s", str(cvewebObj.getKernelGitCommits()))
